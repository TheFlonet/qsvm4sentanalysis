\chapter{Future works}\label{sec:future}

Understanding the limitations of current QPUs and the pre-processing procedures associated with their use is essential to ensure their informed use. Recognising strengths and weaknesses enables a more conscious assessment of when and how to deploy quantum resources for a given problem.

Although the QPU topology may pose a constraint in developing efficient solutions, this is a structural limitation that cannot be circumvented. The development of new QPUs is currently being explored by D-Wave, where it is reasonable to assume that technical experts accustomed to working closely with hardware are heavily involved.

From a computer science perspective, it is possible to ``limit'' ourselves to understand the available resources and leverage current tools most effectively. For this reason, the primary research directions that merit further investigation, beyond the work conducted in this thesis, include the following:

\begin{itemize}
    \item Reduction of the SVM problem into QUBO form, resulting in optimisation to classify examples from sentiment analysis (Section \ref{sec:qnlp});
    \item Developing alternative hybrid solvers (Section \ref{sec:bettersolver}).
\end{itemize}

\section{Quantum Natural Language Processing}\label{sec:qnlp}

Utilizing quantum annealing processes to optimize the support vector machine model has proven to be a successful strategy compared to state-of-the-art classical optimizers. Although the contribution of the QPU is marginal according to the data gathered in Section \ref{sec:qpuusage}, D-Wave's proprietary technology prevents further investigation. This is a significant limitation, suggesting that future research should not focus on methods to accelerate the training of models using D-Wave's solvers.

Since Section \ref{sec:dataset-full} revealed that increasing the number of examples does not correspond to an improvement in performance, reducing the size of the input dataset could be a way further to reduce the training time of the quantum annealing-based model. This would lead to:
\begin{itemize}
    \item A reduction in training times;
    \item A model that is faster to create and transfer to the cloud infrastructure due to the smaller size.
\end{itemize}
It remains to be verified to what extent the dataset size can be reduced without degrading the model's performance.

The performance of the quantum model when compared to state-of-the-art Transformer models is a different matter. As discussed in Section \ref{sec:qsvm-performance}, the difference between the proposed solution and RoBERTa is substantial, making the quantum approach unsuitable for cases where the cost of inference errors outweighs the longer training times required by the model.

The main directions for improving the proposed model include:
\begin{itemize}
    \item Investigating model parameterization;
    \item Using different textual embeddings;
    \item Leveraging SVMs in deep learning contexts;
    \item Exploring alternative models.
\end{itemize}

\paragraph{Problem Parameterization} As with the dataset size used in training, the model's parameterization was not deeply explored during the development of this thesis.

The parameter $C$ (Section \ref{sec:soft}) was initially set to $255$ because:
\begin{itemize}
    \item The performance obtained was significantly better than random guessing;
    \item The focus of the thesis was on studying quantum solvers.
\end{itemize}
No other parameter combinations were tested, but finding an optimal value for $C$ could improve the modelâ€™s performance by a few percentage points.

It should be noted, however, that even with improved performance, different parameterizations are unlikely to surpass the results achieved by RoBERTa, given the limitations of the SVM model when applied to linguistic tasks.

\paragraph{Exploring Better Embeddings} Section \ref{sec:embeddingused} defined the procedure used to generate textual embeddings.

To fully understand the limitations of the SVM model, it is necessary to estimate the contribution of the embedding to the classification task. Comparing model performance using different embeddings, while also considering the time required to produce them, may provide a valuable avenue for further investigation.

Possible alternative embeddings could include:
\begin{itemize}
    \item Embeddings derived from more task-specific models, specifically developed for sentiment analysis;
    \item Embeddings generated by aggregating the information on the word level, instead of using the current procedure that directly generates an embedding for the entire sentence;
    \item Manually constructed embeddings based on descriptive features obtained from an exploratory dataset analysis.
\end{itemize}

\paragraph{Multiple SVM Approaches} Instead of using a single SVM for classification, strategies involving an ensemble of models could be employed. If the number of trained SVMs is kept sufficiently low, training multiple models would still be significantly faster than the time required for Transformers, potentially achieving comparable performance.

The main methodologies that could be explored include:
\begin{itemize}
    \item Implementing an ensemble learning algorithm\cite{adaboost}, which unlike the experiments conducted, provides each model in the ensemble with a different partition of the dataset;
    \item Deep learning approaches based on SVMs, such as those proposed by Jingyuan Wang et al. in \cite{deepsvm}.
\end{itemize}

\paragraph{Alternative Models} All previous proposals are based on improving the SVM model and finding methods to enhance its performance or expressive capacity.

Although the dual formulation of SVMs makes quantum annealing a natural fit, it is possible to study the reduction of alternative machine learning and deep learning models into quadratic programming problems. The best way to close the gap between QSVM and RoBERTa may lie in adopting a new model. In this case, it would be necessary to evaluate:
\begin{itemize}
    \item The cost of reformulating the problem;
    \item The complexity of the resulting model, as different models may be too complex for current hybrid solvers.
\end{itemize}

\section{Improving Current Quantum Solvers}\label{sec:bettersolver}

The second area where this thesis could be further developed involves \texttt{QSplitSampler}, the open-source hybrid solver developed as an alternative to D-Wave's offerings.

The proposed algorithm has room for improvement in both its use of quantum resources and its execution of local code.

\paragraph{Classical Component} Although the current code is reasonably efficient and does not represent the main bottleneck during execution, it could still be optimized.

\subparagraph{Vectorization of Calculations} Currently, calculations are performed as outlined in Section \ref{sec:qsplitimplementation}. By leveraging vectorization techniques such as SIMD (Single Instruction Multiple Data), repetitive operations can be handled more efficiently using modern CPU architectures.

\subparagraph{Parallelizing the Procedure} Since the splitting process generates subproblems that can be handled independently, it is possible to parallelize the recursive subdivision, awaiting results for the aggregation phase. However, it should be noted that part of the performance gains from parallelization may be offset by the quantum component. Once the problems are sent to D-Wave's cloud infrastructure, they are queued and solved one at a time. This behaviour limits the potential of parallel programming. It may still be worth further investigation since QPU solutions appear fast enough to potentially reduce average wait times if D-Wave is provided with multiple problems to solve in sequence.

\paragraph{Quantum Component} While it is possible to start ``from scratch'' to develop new open-source solvers capable of leveraging different properties of the problems being tackled, it could be interesting from a research perspective to explore the limits of \texttt{QSplitSampler} by improving some ``critical'' aspects that emerged during its development and use.

\subparagraph{Diversifying Partitioning Strategies} As an alternative to the current approach, QUBO problems could be divided based on the structure of the problem. For example, in cases where a significant portion of the problem consists only of zero coefficients, the subproblem size could be increased, as the resulting graph would remain small enough to compute the minor embedding and execute on the currently available QPUs.

\subparagraph{Different Allocation Between CPU and QPU} Currently, all subproblems generated by \texttt{QSplitSampler} are sent to the QPU. By reducing the quantum contribution in the solution search, it could be considered a quick analysis of problems to determine whether to send them to the QPU or solve them using the CPU. As with the development of an alternative partitioning strategy, the main challenge with this approach would be finding an efficient heuristic to evaluate the problems without negatively impacting the overall performance of the algorithm.

\subparagraph{Incorporating \texttt{QSplitSampler} into Hybrid Pipelines} Creating pipelines represents a special case of workload distribution between CPU and QPU. Even assuming \texttt{QSplitSampler} remains unchanged, the solutions it produces could be used as starting points for local search techniques. This approach was originally used by D-Wave in the now-deprecated \texttt{QBSolv}\footnote{\url{https://github.com/dwavesystems/qbsolv}} library. Alternating quantum search processes with classical searches in the state space could be a compromise capable of accelerating the search without negatively impacting solution quality.
